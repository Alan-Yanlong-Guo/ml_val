{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import linalg, optimize\n",
    "from sklearn import linear_model, tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-d argmax\n",
    "def fw1(x):\n",
    "    d = len(x)\n",
    "    m = np.max(x)\n",
    "    for i in range(d):\n",
    "        if x[i] == m:\n",
    "            return i\n",
    "\n",
    "# 2-d argmax\n",
    "def fw2(x):\n",
    "    d = x.shape\n",
    "    m = np.max(x)\n",
    "    for i in range(d[0]):\n",
    "        for j in range(d[1]):\n",
    "            if x[i, j] == m:\n",
    "                return [i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(pd.read_csv('Data/Russell.csv', chunksize = 10000))\n",
    "ts = pd.read_csv('Data/tspredictors_1950.csv')\n",
    "# Downcast to lower memory usage\n",
    "df = df.apply(pd.to_numeric, downcast = 'float')\n",
    "df = df.apply(pd.to_numeric, downcast = 'unsigned')\n",
    "ts = ts.apply(pd.to_numeric, downcast = 'float')\n",
    "\n",
    "df['year'] = df['date'].astype(str).str[0:4].astype('uint16')\n",
    "df['month'] = df['date'].astype(str).str[4:6].astype('uint32')\n",
    "df['year_month'] = df['date'].astype(str).str[0:6].astype('uint32')\n",
    "years = np.arange(1957, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = df.merge(ts, how = 'left', left_on = 'year_month', right_on = 'date', suffixes = ('_x', '')).iloc[:, 102:]\n",
    "macro['C'] = 1\n",
    "cols = ['C', 'dp', 'ep', 'bm', 'ntis', 'tbl', 'tms', 'dfy', 'svar'] #constant + 8 aggregate ts variable\n",
    "macro = macro[cols]\n",
    "\n",
    "y = df['ret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_MaxFeatures_Depth(x_train, y_train, x_val, y_val,mtrain, result_path, model_path):\n",
    "    depth = range(1,6,1)  # depth\n",
    "    max_features = range(20,65,5)\n",
    "    ne = 100  \n",
    "    r_oos = np.zeros((len(depth), len(max_features))) #r_oos is validation r2, same for all other models\n",
    "    r_train = np.zeros((len(depth), len(max_features)))\n",
    "    \n",
    "    #Optimizing Model\n",
    "    for n1, d in enumerate(tqdm(depth)): \n",
    "        for n2, mf in enumerate(max_features):\n",
    "            model = RandomForestRegressor(max_depth = d, \n",
    "                                            n_estimators = ne, \n",
    "                                            max_features = mf,\n",
    "                                            random_state = seed,\n",
    "                                            n_jobs = -1)\n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            y_hat = model.predict(x_val)\n",
    "            r_oos[n1, n2] = 1 - sum(np.power(y_hat - y_val, 2))/sum(np.power(y_val - mtrain,2))\n",
    "            r_train[n1, n2] = model.score(x_train, y_train)\n",
    "    \n",
    "    pd.DataFrame(r_oos).to_csv(result_path+'r2_oos'+str(years[25+loop])+'.csv')\n",
    "    pd.DataFrame(r_train).to_csv(result_path+'r2_train'+str(years[25+loop])+'.csv')\n",
    "    \n",
    "    best_d = depth[int(fw2(r_oos)[0])]\n",
    "    best_mf = max_features[int(fw2(r_oos)[1])]\n",
    "    \n",
    "    #Build optimized model\n",
    "    optimized_model = RandomForestRegressor(max_depth = best_d, \n",
    "                                            n_estimators = ne,\n",
    "                                            max_features = best_mf,\n",
    "                                            random_state = seed,\n",
    "                                            n_jobs = -1)\n",
    "    optimized_model.fit(x_train, y_train)\n",
    "    pickle.dump(optimized_model, open(model_path+str(years[25+loop])+'.sav', 'wb'))\n",
    "    \n",
    "    return (best_d, best_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_MaxFeatures_MinSamples(x_train, y_train, x_val, y_val,mtrain, result_path, model_path):\n",
    "    min_samples = range(1,11,1)  \n",
    "    max_features = range(20,65,5)\n",
    "    ne = 100  \n",
    "    r_oos = np.zeros((len(min_samples), len(max_features)))\n",
    "    r_train = np.zeros((len(min_samples), len(max_features)))\n",
    "    \n",
    "    #Optimizing Model\n",
    "    for n1, ms in enumerate(tqdm(min_samples)): \n",
    "        for n2, mf in enumerate(max_features):\n",
    "            model = RandomForestRegressor(max_depth = 2, \n",
    "                                            n_estimators = ne, \n",
    "                                            min_samples_leaf = ms,\n",
    "                                            max_features = mf,\n",
    "                                            random_state = seed,\n",
    "                                            n_jobs = -1)\n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            y_hat = model.predict(x_val)\n",
    "            r_oos[n1, n2] = 1 - sum(np.power(y_hat - y_val, 2))/sum(np.power(y_val - mtrain,2))\n",
    "            r_train[n1, n2] = model.score(x_train, y_train)\n",
    "    \n",
    "    pd.DataFrame(r_oos).to_csv(result_path+'r2_oos'+str(years[25+loop])+'.csv')\n",
    "    pd.DataFrame(r_train).to_csv(result_path+'r2_train'+str(years[25+loop])+'.csv')\n",
    "    \n",
    "    best_ms = min_samples[int(fw2(r_oos)[0])]\n",
    "    best_mf = max_features[int(fw2(r_oos)[1])]\n",
    "    \n",
    "    #Build optimized model\n",
    "    optimized_model = RandomForestRegressor(max_depth = 2, \n",
    "                                            n_estimators = ne,\n",
    "                                            min_samples_leaf = best_ms,\n",
    "                                            max_features = best_mf,\n",
    "                                            random_state = seed,\n",
    "                                            n_jobs = -1)\n",
    "    optimized_model.fit(x_train, y_train)\n",
    "    pickle.dump(optimized_model, open(model_path+str(years[25+loop])+'.sav', 'wb'))\n",
    "    \n",
    "    return (best_ms, best_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_MinSamples_Depth(x_train, y_train, x_val, y_val, mtrain, result_path, model_path):\n",
    "    depth = range(1,6,1) \n",
    "    min_samples = range(1,11,1)\n",
    "    ne = 100  \n",
    "    r_oos = np.zeros((len(depth), len(min_samples)))\n",
    "    r_train = np.zeros((len(depth), len(min_samples)))\n",
    "    \n",
    "    #Optimizing Model\n",
    "    for n1, d in enumerate(tqdm(depth)): \n",
    "        for n2, mf in enumerate(min_samples):\n",
    "            model = RandomForestRegressor(max_depth = d, \n",
    "                                            n_estimators = ne,\n",
    "                                            min_samples_leaf = ms,\n",
    "                                            max_features = 'sqrt',\n",
    "                                            random_state = seed,\n",
    "                                            n_jobs = -1)\n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            y_hat = model.predict(x_val)\n",
    "            r_oos[n1, n2] = 1 - sum(np.power(y_hat - y_val, 2))/sum(np.power(y_val - mtrain,2))\n",
    "            r_train[n1, n2] = model.score(x_train, y_train)\n",
    "    \n",
    "    pd.DataFrame(r_oos).to_csv(result_path+'r2_oos'+str(years[25+loop])+'.csv')\n",
    "    pd.DataFrame(r_train).to_csv(result_path+'r2_train'+str(years[25+loop])+'.csv')\n",
    "    \n",
    "    best_d = depth[int(fw2(r_oos)[0])]\n",
    "    best_ms = min_samples[int(fw2(r_oos)[1])]\n",
    "    \n",
    "    #Build optimized model\n",
    "    optimized_model = RandomForestRegressor(max_depth = best_d, \n",
    "                                            n_estimators = ne,\n",
    "                                            min_samples_leaf = best_ms,\n",
    "                                            max_features = 'sqrt',\n",
    "                                            random_state = seed,\n",
    "                                            n_jobs = -1)\n",
    "    optimized_model.fit(x_train, y_train)\n",
    "    print(optimized_model.feature_importances_)\n",
    "    pickle.dump(optimized_model, open(model_path+str(years[25+loop])+'.sav', 'wb'))\n",
    "    \n",
    "    return (best_d, best_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, optimizing, and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "for loop in range(34):\n",
    "    train_idx = df.index[df['year'].isin(years[:(15 + loop)])]\n",
    "    val_idx = df.index[df['year'].isin(years[(15 + loop):(25 + loop)])]\n",
    "\n",
    "    #Using features without interaction with macro features   \n",
    "    x_train = df.iloc[train_idx, 0:94]*macro.iloc[train_idx, 0][:, np.newaxis]\n",
    "    x_val = df.iloc[val_idx, 0:94]*macro.iloc[val_idx, 0][:, np.newaxis]\n",
    "    x_train = pd.concat((x_train, pd.get_dummies(df['sic2'].astype(str), prefix = 'sic2').iloc[train_idx, :-1]), axis = 1)\n",
    "    x_val = pd.concat((x_val, pd.get_dummies(df['sic2'].astype(str), prefix = 'sic2').iloc[val_idx, :-1]), axis = 1)\n",
    "\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    y_oos = y[oos_idx]\n",
    "    mtrain = np.mean(y_train)\n",
    "    \n",
    "    print(\"X_train shape:\",x_train.shape)\n",
    "    print(\"X_val shape: \",x_val.shape)\n",
    "    \n",
    "    RF_MaxFeatures_Depth(x_train,y_train,x_val,y_val,mtrain,'Results/RF/MF_D/','Models/RF/MF_D/')\n",
    "    RF_MaxFeatures_MinSamples(x_train,y_train,x_val,y_val,mtrain,'Results/RF/MF_MS/','Models/RF/MF_MS/')\n",
    "    RF_MinSamples_Depth(x_train,y_train,x_val,y_val,mtrain,'Results/RF/MS_D/','Models/RF/MS_D/')\n",
    "\n",
    "    \n",
    "    #Using Full Set Features\n",
    "    x_train = df.iloc[train_idx, 0:94]*macro.iloc[train_idx, 0][:, np.newaxis]\n",
    "    x_val = df.iloc[val_idx, 0:94]*macro.iloc[val_idx, 0][:, np.newaxis]\n",
    "    for i in range(1, macro.shape[1]):\n",
    "        x_train = pd.concat((x_train, df.iloc[train_idx, 0:94]*macro.iloc[train_idx, i][:, np.newaxis]), axis = 1)\n",
    "        x_val = pd.concat((x_val, df.iloc[val_idx, 0:94]*macro.iloc[val_idx, i][:, np.newaxis]), axis = 1)\n",
    "        \n",
    "    x_train.columns = [a + '*' + b for b in macro.columns for a in df.columns[0:94]]\n",
    "    x_val.columns = [a + '*' + b for b in macro.columns for a in df.columns[0:94]]\n",
    "    \n",
    "    x_train = pd.concat((x_train, pd.get_dummies(df['sic2'].astype(str), prefix = 'sic2').iloc[train_idx, :-1]), axis = 1)\n",
    "    x_val = pd.concat((x_val, pd.get_dummies(df['sic2'].astype(str), prefix = 'sic2').iloc[val_idx, :-1]), axis = 1)\n",
    "    \n",
    "    print(\"X_train shape:\",x_train.shape)\n",
    "    print(\"X_val shape: \",x_val.shape)\n",
    "    \n",
    "    RF_MaxFeatures_Depth(x_train,y_train,x_val,y_val,mtrain,'Results/RF/MF_D_Full/','Models/RF/MF_D_Full/')\n",
    "    RF_MaxFeatures_MinSamples(x_train,y_train,x_val,y_val,mtrain,'Results/RF/MF_MS_Full/','Models/RF/MF_MS_Full/')\n",
    "    RF_MinSamples_Depth(x_train,y_train,x_val,y_val,mtrain,'Results/RF/MS_D_Full/','Models/RF/MS_D_Full/')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Model Below\n",
    "\n",
    "Parameters\n",
    "n_estimators (int) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "\n",
    "max_depth (int) – Maximum tree depth for base learners.\n",
    "\n",
    "learning_rate (float) – Boosting learning rate (xgb’s “eta”)\n",
    "\n",
    "verbosity (int) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "\n",
    "objective (string or callable) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n",
    "\n",
    "booster (string) – Specify which booster to use: gbtree, gblinear or dart.\n",
    "\n",
    "tree_method (string) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from parameters document.\n",
    "\n",
    "n_jobs (int) – Number of parallel threads used to run xgboost.\n",
    "\n",
    "gamma (float) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "\n",
    "min_child_weight (int) – Minimum sum of instance weight(hessian) needed in a child.\n",
    "\n",
    "max_delta_step (int) – Maximum delta step we allow each tree’s weight estimation to be.\n",
    "\n",
    "subsample (float) – Subsample ratio of the training instance.\n",
    "\n",
    "colsample_bytree (float) – Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "colsample_bylevel (float) – Subsample ratio of columns for each level.\n",
    "\n",
    "colsample_bynode (float) – Subsample ratio of columns for each split.\n",
    "\n",
    "reg_alpha (float (xgb's alpha)) – L1 regularization term on weights\n",
    "\n",
    "reg_lambda (float (xgb's lambda)) – L2 regularization term on weights\n",
    "\n",
    "scale_pos_weight (float) – Balancing of positive and negative weights.\n",
    "\n",
    "base_score – The initial prediction score of all instances, global bias.\n",
    "\n",
    "random_state (int) –\n",
    "\n",
    "\n",
    "eta typical value 0.01-0.2\n",
    "min_child_weight controls overfitting, higher will be more generalization, minimum sum of weights\n",
    "max_depth controls overfitting typical 3-10\n",
    "gamma specifies minimum threshold for loss reduction\n",
    "subsample, denote fraction of observations to be randomly samples, 0.5-1\n",
    "lambda L2 regularization\n",
    "alpha L1 Reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_base_model(x_train, y_train, x_val, y_val, mtrain, result_path, model_path):\n",
    "    learning_rates = [-1,-0.8,-0.6,-0.4,-0.2,0]\n",
    "    max_depth = range(1,6,1)\n",
    "    ne = 100  \n",
    "    r_oos = np.zeros((len(learning_rates), len(max_depth)))\n",
    "    r_train = np.zeros((len(learning_rates), len(max_depth)))\n",
    "    \n",
    "    for n1,llr in enumerate(tqdm(learning_rates)):\n",
    "        for n2, d in enumerate(tqdm(max_depth)):\n",
    "            lr = 10 ** llr\n",
    "            model = xgb.XGBRegressor(max_depth = d, \n",
    "                                    n_estimators = ne, \n",
    "                                    learning_rate = lr,\n",
    "                                    objective = 'reg:squarederror',\n",
    "                                    reg_alpha = 0,\n",
    "                                    reg_lambda = 0,\n",
    "                                    base_score = mtrain,\n",
    "                                    random_state = 7,\n",
    "                                    n_jobs = -1)\n",
    "            \n",
    "            model.fit(x_train, y_train, eval_set=[(x_val, y_val)], eval_metric = 'rmse', verbose = False, early_stopping_rounds=25)\n",
    "            y_hat = model.predict(x_val)\n",
    "            r_oos[n1, n2] = 1 - sum(np.power(y_hat - y_val, 2))/sum(np.power(y_val - mtrain,2))\n",
    "            r_train[n1, n2] = model.score(x_train, y_train)\n",
    "            \n",
    "    pd.DataFrame(r_oos).to_csv(result_path+'r2_oos'+str(years[25+loop])+'.csv')\n",
    "    pd.DataFrame(r_train).to_csv(result_path+'r2_train'+str(years[25+loop])+'.csv')\n",
    "    \n",
    "    best_lr = learning_rates[int(fw2(r_oos)[0])]\n",
    "    best_d = max_depth[int(fw2(r_oos)[1])]\n",
    "    \n",
    "    #Build optimized model\n",
    "    optimized_model = xgb.XGBRegressor(max_depth = best_d, \n",
    "                                    n_estimators = ne, \n",
    "                                    learning_rate = 10**best_lr,\n",
    "                                    objective = 'reg:squarederror',\n",
    "                                    reg_alpha = 0,\n",
    "                                    reg_lambda = 0,\n",
    "                                    base_score = mtrain,\n",
    "                                    random_state = 7,\n",
    "                                    n_jobs = -1)\n",
    "    \n",
    "    optimized_model.fit(x_train, y_train,  eval_set=[(x_val, y_val)], eval_metric = 'rmse', verbose = True, early_stopping_rounds=25)\n",
    "    pickle.dump(optimized_model, open(model_path+str(years[25+loop])+'.sav', 'wb'))\n",
    "    \n",
    "    return (best_lr, best_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first run use max_depth = 3, lr = 0.2  range(0.0,0.5,0.05)\n",
    "#second run use max_depth = 1, lr = 0.4 range(0.2,0.7,0.1)\n",
    "#third run use max_depth = 3, lr = 0.2 range(0.2,0.7,0.1)\n",
    "def XGB_alpha_lambda_model(x_train, y_train, x_val, y_val, mtrain, result_path, model_path):\n",
    "    alphas = np.arange(0.2,0.7,0.1)\n",
    "    lambdas = np.arange(0.2,0.7,0.1)\n",
    "    ne = 100  \n",
    "    r_oos = np.zeros((len(alphas), len(lambdas)))\n",
    "    r_train = np.zeros((len(alphas), len(lambdas)))\n",
    "    \n",
    "    for n1,alpha in enumerate(tqdm(alphas)):\n",
    "        for n2, l in enumerate(tqdm(lambdas)):\n",
    "            model = xgb.XGBRegressor(max_depth = 3, \n",
    "                                    n_estimators = ne, \n",
    "                                    learning_rate = 0.2,\n",
    "                                    objective = 'reg:squarederror',\n",
    "                                    reg_alpha = alpha,\n",
    "                                    reg_lambda = l,\n",
    "                                    base_score = mtrain,\n",
    "                                    random_state = 7,\n",
    "                                    n_jobs = -1)\n",
    "            \n",
    "            model.fit(x_train, y_train, eval_set=[(x_val, y_val)], eval_metric = 'rmse', verbose = False, early_stopping_rounds=25)\n",
    "            y_hat = model.predict(x_val)\n",
    "            r_oos[n1, n2] = 1 - sum(np.power(y_hat - y_val, 2))/sum(np.power(y_val - mtrain,2))\n",
    "            r_train[n1, n2] = model.score(x_train, y_train)\n",
    "            \n",
    "    pd.DataFrame(r_oos).to_csv(result_path+'r2_oos'+str(years[25+loop])+'.csv')\n",
    "    pd.DataFrame(r_train).to_csv(result_path+'r2_train'+str(years[25+loop])+'.csv')\n",
    "    \n",
    "    best_alpha = alphas[int(fw2(r_oos)[0])]\n",
    "    best_lambda = lambdas[int(fw2(r_oos)[1])]\n",
    "    \n",
    "    #Build optimized model\n",
    "    optimized_model = xgb.XGBRegressor(max_depth = 3, \n",
    "                                    n_estimators = ne, \n",
    "                                    learning_rate = 0.2,\n",
    "                                    objective = 'reg:squarederror',\n",
    "                                    reg_alpha = best_alpha,\n",
    "                                    reg_lambda = best_lambda,\n",
    "                                    base_score = mtrain,\n",
    "                                    random_state = 7,\n",
    "                                    n_jobs = -1)\n",
    "    \n",
    "    optimized_model.fit(x_train, y_train,  eval_set=[(x_val, y_val)], eval_metric = 'rmse', verbose = True, early_stopping_rounds=25)\n",
    "    pickle.dump(optimized_model, open(model_path+str(years[25+loop])+'.sav', 'wb'))\n",
    "    \n",
    "    return (best_alpha, best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_sample_ratio_model(x_train, y_train, x_val, y_val, mtrain, result_path, model_path):\n",
    "    sample_ratios = np.arange(0.5,1.0,0.1)\n",
    "    \n",
    "    #lambdas = np.arange(0.2,0.7,0.1)\n",
    "    ne = 100  \n",
    "    r_oos = np.zeros((len(sample_ratios), len(sample_ratios)))\n",
    "    r_train =  np.zeros((len(sample_ratios), len(sample_ratios)))\n",
    "    \n",
    "    for n1, sample_ratio in enumerate(tqdm(sample_ratios)):\n",
    "        for n2, col_ratio in enumerate(tqdm(sample_ratios)):\n",
    "            model = xgb.XGBRegressor(max_depth = 1, \n",
    "                                    n_estimators = ne, \n",
    "                                    learning_rate = 0.4,\n",
    "                                    objective = 'reg:squarederror',\n",
    "                                    reg_alpha = 0.5,\n",
    "                                    reg_lambda = 0.5,\n",
    "                                    subsample = sample_ratio,\n",
    "                                    colsample_bytree = col_ratio,\n",
    "                                    base_score = mtrain,\n",
    "                                    random_state = 7,\n",
    "                                    n_jobs = -1)\n",
    "            \n",
    "            model.fit(x_train, y_train, eval_set=[(x_val, y_val)], eval_metric = 'rmse', verbose = False, early_stopping_rounds=20)\n",
    "            y_hat = model.predict(x_val)\n",
    "            r_oos[n1, n2] = 1 - sum(np.power(y_hat - y_val, 2))/sum(np.power(y_val - mtrain,2))\n",
    "            r_train[n1, n2] = model.score(x_train, y_train)\n",
    "            \n",
    "    pd.DataFrame(r_oos).to_csv(result_path+'r2_oos'+str(years[25+loop])+'.csv')\n",
    "    pd.DataFrame(r_train).to_csv(result_path+'r2_train'+str(years[25+loop])+'.csv')\n",
    "    \n",
    "    best_sample_ratio = sample_ratios[int(fw2(r_oos)[0])]\n",
    "    best_col_ratio = sample_ratios[int(fw2(r_oos)[1])]\n",
    "    \n",
    "    #Build optimized model\n",
    "    optimized_model = xgb.XGBRegressor(max_depth = 1, \n",
    "                                    n_estimators = ne, \n",
    "                                    learning_rate = 0.4,\n",
    "                                    objective = 'reg:squarederror',\n",
    "                                    reg_alpha = 0.5,\n",
    "                                    reg_lambda = 0.5,\n",
    "                                    subsample = best_sample_ratio,\n",
    "                                    colsample_bytree = best_col_ratio,\n",
    "                                    base_score = mtrain,\n",
    "                                    random_state = 7,\n",
    "                                    n_jobs = -1)\n",
    "    \n",
    "    optimized_model.fit(x_train, y_train,  eval_set=[(x_val, y_val)], eval_metric = 'rmse', verbose = True, early_stopping_rounds=20)\n",
    "    pickle.dump(optimized_model, open(model_path+str(years[25+loop])+'.sav', 'wb'))\n",
    "    \n",
    "    return (best_sample_ratio, best_col_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "for loop in range(34):\n",
    "    train_idx = df.index[df['year'].isin(years[:(15 + loop)])]\n",
    "    val_idx = df.index[df['year'].isin(years[(15 + loop):(25 + loop)])]\n",
    "    \n",
    "    x_train = df.iloc[train_idx, 0:94]*macro.iloc[train_idx, 0][:, np.newaxis]\n",
    "    x_val = df.iloc[val_idx, 0:94]*macro.iloc[val_idx, 0][:, np.newaxis]\n",
    "\n",
    "    x_train = pd.concat((x_train, pd.get_dummies(df['sic2'].astype(str), prefix = 'sic2').iloc[train_idx, :-1]), axis = 1)\n",
    "    x_val = pd.concat((x_val, pd.get_dummies(df['sic2'].astype(str), prefix = 'sic2').iloc[val_idx, :-1]), axis = 1)\n",
    "\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    mtrain = np.mean(y_train)\n",
    "    \n",
    "    print(\"X_train shape:\",x_train.shape)\n",
    "    print(\"X_val shape: \",x_val.shape)\n",
    "    \n",
    "    XGB_sample_ratio_model(x_train, y_train, x_val, y_val, mtrain,'Results/XGBoost/SR_CR/','Models/XGBoost/SR_CR/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
